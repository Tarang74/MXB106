%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages and macros
\usepackage{changepage} % Modify page width
\usepackage{multicol} % Use multiple columns
\usepackage{titlesec} % Modify section heading styles

%% A4 page
\geometry{
	a4paper,
	margin = 10mm
}

%% Hide horizontal rule
\renewcommand{\headrulewidth}{0pt}

% Hide page numbers
\pagenumbering{gobble}

%% Multi-columns setup
\setlength{\columnsep}{4pt}

%% Paragraph setup
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

%% Customise section heading styles
\titleformat*\section{\raggedright\bfseries}

\begin{document}
% Modify spacing
\titlespacing*\section{0pt}{1ex}{1ex}
%
\setlength{\textfloatsep}{0pt}
%
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
%
\begin{minipage}[t]{62.39259259mm}
    \section*{Vector Spaces}
    A \textbf{vector space} \(V\) is closed under vector addition
    and scalar multiplication:
    \begin{equation*}
        \symbf{u}+\symbf{v} \in V \ \text{and} \ k\symbf{u} \in V.
    \end{equation*}
    A \textit{subset} \(W\) of a vector space \(V\) is called a
    \textbf{subspace} of \(V\) if \(W\) is itself a vector space. The
    intersection of subspaces is also a subspace of \(V\).

    \(S\) is \textbf{linearly independent (LI)} if
    \begin{equation*}
        k_1 \symbf{v}_1 + k_2 \symbf{v}_2 + \cdots + k_n \symbf{v}_n = \symbf{0}
    \end{equation*}
    has \(k_i=0\).

    \(S\) forms a \textbf{basis} for \(V\) if \(S\) spans \(V\) and \(S\) is LI\@.
    \section*{Fundamental Subspaces}
    For \(\symbf{A}\in\mathbb{R}^{m \times n}\):
    \begin{itemize}
        \item \(r = \vrank{\left( \symbf{A} \right)} = \dim{\left( \columnspace{A} \right)}\)
        \item \(r = \vrank{\left( \symbf{A}^\top \right)} = \dim{\left( \rowspace{A} \right)}\)
        \item \(n - r = \vnull{\left( \symbf{A} \right)}      = \dim{\left( \nullspace{A} \right)}\)
        \item \(m - r = \vnull{\left( \symbf{A}^\top \right)} = \dim{\left( \leftnullspace{A} \right)}\)
    \end{itemize}
    \textbf{Row equivalent} matrices have the same
    \underline{row space} and \underline{null space}.
    \section*{Orthogonality}
    The subspaces \(U\) and \(W\) of a vector space \(V\) are
    \textbf{orthogonal subspaces} iff
    \begin{equation*}
        \forall \symbf{u}\in U:\forall \symbf{w}\in W:\symbf{u}^{\top}\symbf{w} = 0.
    \end{equation*}
    \begin{itemize}
        \item \(\symbf{v}^\top \symbf{v}=\norm{\symbf{v}}^2\)
    \end{itemize}
    The \textbf{orthogonal complement} of \(U\):
    \begin{equation*}
        U^{\perp} = \left\{ \forall \symbf{u}\in U:\symbf{v}\in V: \symbf{v}^{\top}\symbf{u}=0 \right\}
    \end{equation*}
    \begin{itemize}
        \item \(\left( U^{\perp} \right)^{\perp} = U\)
        \item \(\dim{U} + \dim{U^{\perp}} = \dim{V}\)
        \item \(\left( \columnspace{A} \right)^\perp = \leftnullspace{A}\)
        \item \(\left( \rowspace{A} \right)^\perp = \nullspace{A}\)
    \end{itemize}
    \textbf{Projections}:
    \begin{equation*}
        \proj_{\symbf{a}}\symbf{b}
        = \symbf{a} x
        = \symbf{a} \frac{\symbf{a}^\top \symbf{b}}{\symbf{a}^\top \symbf{a}}.
    \end{equation*}
    \begin{gather*}
        \forall \symbf{w}\in W:\symbf{w}\neq \symbf{p}: \\
        \proj_{W}\symbf{b} = \symbf{A}\symbf{\hat{x}} = \symbf{A}\left( \symbf{A}^\top \symbf{A} \right)^{-1}\symbf{A}^\top \symbf{b}: \\
        \norm{\symbf{b}-\symbf{p}}<\norm{\symbf{b}-\symbf{w}}.
    \end{gather*}
    \section*{Determinants}
    \begin{equation*}
        \det{\left( \symbf{A} \right)} = \sum_{j=1}^n a_{ij}\symbf{C}_{ij} = \sum_{i=1}^n a_{ij}\symbf{C}_{ij}
    \end{equation*}
    where \(\symbf{C}_{ij}=\left( -1 \right)^{i+j}\symbf{M}_{ij}\).
    \begin{equation*}
        \symbf{A}^{-1}=\frac{1}{\det{\symbf{A}}} \adj{\left( \symbf{A} \right)}
    \end{equation*}
    where \(\adj{\left( \symbf{A} \right)}=\symbf{C}^\top\).
\end{minipage}\hfill%
\begin{minipage}[t]{126.1962963mm}
    \section*{Linear Maps}
    \textbf{Linear transformations}:
    \begin{gather*}
        T:V\rightarrow W \iff \forall \symbf{u},\: \symbf{v} \in V:\forall k \in \mathbb{R}: \\
        T\left(\symbf{u}+\symbf{v}\right) = T\left(\symbf{u}\right) + T\left(\symbf{v}\right) \wedge T\left(k\symbf{u}\right) = kT\left(\symbf{u}\right)
    \end{gather*}
    \textbf{Rotations}: Anticlockwise looking down from the positive direction of the
    axis of rotation.
    \begin{align*}
        \symbf{R}_x =
        \begin{bmatrix}
            1 & 0                           & 0                            \\
            0 & \cos{\left( \theta \right)} & -\sin{\left( \theta \right)} \\
            0 & \sin{\left( \theta \right)} & \cos{\left( \theta \right)}
        \end{bmatrix}
        \quad \symbf{R}_y =
        \begin{bmatrix}
            \cos{\left( \theta \right)}  & 0 & \sin{\left( \theta \right)} \\
            0                            & 1 & 0                           \\
            -\sin{\left( \theta \right)} & 0 & \cos{\left( \theta \right)}
        \end{bmatrix} \\
        \symbf{R}_z =
        \begin{bmatrix}
            \cos{\left( \theta \right)} & -\sin{\left( \theta \right)} & 0 \\
            \sin{\left( \theta \right)} & \cos{\left( \theta \right)}  & 0 \\
            0                           & 0                            & 1
        \end{bmatrix}
        \quad \symbf{R} =
        \begin{bmatrix}
            \cos{\left( \theta \right)} & -\sin{\left( \theta \right)} \\
            \sin{\left( \theta \right)} & \cos{\left( \theta \right)}
        \end{bmatrix}.
    \end{align*}
    \textbf{Shears}:
    \begin{align*}
        \symbf{S}_x =
        \begin{bmatrix}
            1 & a & b \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}
        \quad \symbf{S}_y =
        \begin{bmatrix}
            1 & 0 & 0 \\
            a & 1 & b \\
            0 & 0 & 1
        \end{bmatrix}
        \quad \symbf{S}_z =
        \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            a & b & 1
        \end{bmatrix}
    \end{align*}
    where the standard basis vector in the subscripted axis maps to itself.
    Think about where the standard basis vectors maps.

    \textbf{Reflections}:
    \begin{align*}
        \symbf{M}_{xy}=
        \begin{bmatrix}
            1 & 0 & 0  \\
            0 & 1 & 0  \\
            0 & 0 & -1
        \end{bmatrix}
        \quad \symbf{M}_{xz} =
        \begin{bmatrix}
            1 & 0  & 0 \\
            0 & -1 & 0 \\
            0 & 0  & 1
        \end{bmatrix}
        \quad \symbf{M}_{yz} =
        \begin{bmatrix}
            -1 & 0 & 0 \\
            0  & 1 & 0 \\
            0  & 0 & 1
        \end{bmatrix}
    \end{align*}
    where vectors are reflected across the plane formed by the subscripts of
    \(\symbf{M}\).

    \textbf{2d reflections} about \(y=mx + c\), where \(\theta=\arctan{\left( m \right)}\):
    \begin{align*}
        T\left( \symbf{v} \right) & = \symbf{R} \symbf{M}_{xz} \symbf{R}^{-1} \left( \symbf{v} - \begin{bmatrix} 0 \\ c \end{bmatrix} \right) + \begin{bmatrix} 0 \\ c \end{bmatrix} \\
                                    & =
        \begin{bmatrix}
            \cos{\left( \theta \right)} & -\sin{\left( \theta \right)} \\
            \sin{\left( \theta \right)} & \cos{\left( \theta \right)}
        \end{bmatrix}
        \begin{bmatrix}
            1 & 0  \\
            0 & -1
        \end{bmatrix}
        \begin{bmatrix}
            \cos{\left( \theta \right)} & -\sin{\left( \theta \right)} \\
            \sin{\left( \theta \right)} & \cos{\left( \theta \right)}
        \end{bmatrix}^{-1} \left( \symbf{v} - \begin{bmatrix} 0 \\ c \end{bmatrix} \right) +
        \begin{bmatrix} 0 \\ c \end{bmatrix}                                                                                                                                               \\
                                    & = \frac{1}{1+m^2}
        \begin{bmatrix}
            1-m^2 & 2m    \\
            2m    & m^2-1
        \end{bmatrix} \left( \symbf{v} - \begin{bmatrix} 0 \\ c \end{bmatrix} \right) +
        \begin{bmatrix} 0 \\ c \end{bmatrix}                                                                                                                                               \\
    \end{align*}
    \begin{multicols*}{2}
        \section*{Invariant Subspaces}
        \textbf{Invariant (IV) subspaces}:

        For \(T:V\rightarrow V\), \(\mathcal{V}\) is IV if
        \begin{equation*}
            T\left(\mathcal{V}\right)\subseteq \mathcal{V}\iff\forall \symbf{v}
            \in \mathcal{V}\implies T\left(\symbf{v}\right)\in \mathcal{V}.
        \end{equation*}
        \textbf{Trivial IV subspaces}:
        \begin{enumerate}
            \item \(V\)
            \item \(\vim{\left( T \right)} \equiv T(V) = \left\{ T\left(\symbf{v}\right) : \symbf{v}\in V \right\}\)
            \item \(\vker{\left( T \right)} = \left\{ \symbf{v}\in V : T\left(\symbf{v}\right) = \symbf{0} \right\}\)
            \item \(\left\{ \symbf{0} \right\}\)
            \item linear combination of IV subspaces
        \end{enumerate}
        \textbf{Eigenspaces} (1d IV subspace):
        \begin{equation*}
            \mathcal{V} = \left\{ \forall \symbf{v}\in \mathcal{V}:\exists
            \lambda \in \mathbb{C}:T\left(\symbf{v}\right) = \lambda \symbf{v} \right\}
        \end{equation*}
        where \(\lambda_i\) are the eigenvalues of \(\symbf{A}\) and
        \(\symbf{v}_i\) are the eigenvectors of \(\symbf{A}\), and they satisfy
        \(\left( \symbf{A} - \lambda \symbf{I} \right) \symbf{v}=\symbf{0}\).

        If \(\symbf{A}\) is invertible:
        \(\det{\left( \symbf{A} - \lambda\symbf{I} \right)} = 0\).

        \textbf{Characteristic polynomial}:
        \begin{equation*}
            p_n\left(\lambda\right) = \det{\left( \symbf{A}_n - \lambda\symbf{I}_n \right)}
        \end{equation*}
        \textbf{In 2d}:
        \begin{equation*}
            p_2\left(\lambda\right) = \lambda^2-\Tr{\left( \symbf{A} \right)}\lambda + \det{\left( \symbf{A} \right)}
        \end{equation*}
        \begin{equation*}
            \Tr{\left( \symbf{A} \right)} = \sum_{i=1}^n \lambda_i \quad \text{and} \quad
            \det{\left( \symbf{A} \right)} = \prod_{i=1}^n \lambda_i
        \end{equation*}
        \textbf{Similarity transformation}:
        \begin{equation*}
            \symbf{A}\rightarrow \symbf{V}^{-1}\symbf{A}\symbf{V}
        \end{equation*}
        If \(\symbf{v}_i\) are LI, then \(\symbf{A}\) is
        \textbf{diagonalisable}:
        \begin{equation*}
            \symbf{D}=\symbf{V}^{-1}\symbf{A}\symbf{V} \iff \symbf{A}=\symbf{V}\symbf{D}\symbf{V}^{-1}
        \end{equation*}
    \end{multicols*}
    If \(\symbf{A}\) is diagonalisable:
    \begin{equation*}
        \symbf{D}=
        \begin{bmatrix}
            \lambda_1                         \\
             & \lambda_2                      \\
             &           & \ddots             \\
             &           &        & \lambda_n
        \end{bmatrix} \quad
        \symbf{V}=
        \begin{bmatrix}
            \vertbar      & \vertbar      &        & \vertbar      \\
            \symbf{v}_1 & \symbf{v}_2 & \cdots & \symbf{v}_n \\
            \vertbar      & \vertbar      &        & \vertbar
        \end{bmatrix}
    \end{equation*}
    \begin{equation*}
        \forall k \in \mathbb{N}_0:\symbf{A}^k = \symbf{V} \symbf{D}^k \symbf{V}^{-1}
    \end{equation*}
    The eigenvalues of \(\symbf{A}^k\) are the eigenvalues of \(\symbf{A}\)
    to the \(k\)-th power: \(\lambda_1^k,\: \lambda_2^k,\: \dots,\: \lambda_n^k\).

    The eigenvectors of \(\symbf{A}^k\) equal the eigenvectors of \(\symbf{A}\).
\end{minipage}
\section*{Differential Equations}
The \textbf{ordinary differential equation (ODE)}: \(x' = a x\),
has the solution: \(x(t) = c_1 e^{a t}\). \(c_1\) is determined through
initial conditions.
\begin{multicols*}{2}
    The \textbf{system of differential equations}:
    \begin{align*}
             & \begin{cases}
                   \begin{array}{ c >{{}}c<{{}} c >{{}}c<{{}} c >{{}}c<{{}} c >{{}}c<{{}} c  }
                x'_1               & = & a_{11}x_1                         & + & a_{12}x_2                         & + & \cdots & + & a_{1n}x_n                         \\
                x'_2               & = & a_{21}x_1                         & + & a_{22}x_2                         & + & \cdots & + & a_{2n}x_n                         \\
                \vdotswithin{x'_3} &   & \vdotswithin{a_{31}}\phantom{x_1} &   & \vdotswithin{a_{32}}\phantom{x_2} &   &        &   & \vdotswithin{a_{3n}}\phantom{x_n} \\
                x'_n               & = & a_{n1}x_1                         & + & a_{n2}x_2                         & + & \cdots & + & a_{nn}x_n
            \end{array}
               \end{cases}            \\
        \iff
             & \odv{}{t}
        \begin{bmatrix}
            x_1    \\
            x_2    \\
            \vdots \\
            x_n
        \end{bmatrix} =
        \begin{bmatrix}
            a_{11} & a_{12} & \cdots & a_{1n} \\
            a_{21} & a_{22} & \cdots & a_{2n} \\
            \vdots & \vdots &        & \vdots \\
            a_{n1} & a_{n2} & \cdots & a_{nn}
        \end{bmatrix}
        \begin{bmatrix}
            x_1 \\ x_2 \\ \vdots \\ x_n
        \end{bmatrix}                  \\
        \iff & \symbf{x}' = \symbf{A} \symbf{x}
    \end{align*}
    can be solved using \(\symbf{x}=\symbf{V}\symbf{u}\) where
    \(\symbf{V}\) is the matrix that diagonalises \(\symbf{A}\) and
    \(\symbf{u}\) is the solution to \(\symbf{u}' = \symbf{D} \symbf{u}\) where
    \(\symbf{D}\) is the diagonal similarity transformation of \(\symbf{A}\).

    If \(\symbf{A}\) is diagonalisable, then for \(\symbf{x}' = \symbf{A} \symbf{x}\):
    \begin{equation*}
        \symbf{x}(t) = c_1 e^{\lambda_1 t} \symbf{v}_1 + c_2 e^{\lambda_2 t} \symbf{v}_2 + \cdots + c_n e^{\lambda_n t} \symbf{v}_n
    \end{equation*}
    For the \textbf{higher-order linear differential equation}:
    \begin{equation*}
        x^{\left( n \right)} + a_1 x^{\left( n-1 \right)} + \cdots + a_{n-1} x' + a_n x = 0
    \end{equation*}
    define
    \begin{equation*}
        x_1 = x,\: x_2 = x',\: \dots,\: x_n = x^{\left( n-1 \right)}
    \end{equation*}
    and let
    \begin{equation*}
        \symbf{x}=
        \begin{bmatrix}
            x_1 \\ x_2 \\ \cdots \\ x_n
        \end{bmatrix}.
    \end{equation*}
    Then solve the following ODE using diagonalisation:
    \begin{equation*}
        \odv{}{t}
        \begin{bmatrix}
            x_1               \\
            x_2               \\
            \vdotswithin{x_3} \\
            x_n
        \end{bmatrix}
        =
        \begin{bmatrix}
            0      & 1        & 0        & \cdots & 0      \\
            0      & 0        & 1        & \cdots & 0      \\
            \vdots & \vdots   & \vdots   & \ddots & \vdots \\
            0      & 0        & 0        & \cdots & 1      \\
            -a_n   & -a_{n-1} & -a_{n-2} & \cdots & -a_1
        \end{bmatrix}
        \begin{bmatrix}
            x_1               \\
            x_2               \\
            \vdotswithin{x_3} \\
            x_n
        \end{bmatrix}.
    \end{equation*}
    \section*{Vector Operations}
    \textbf{Norm of a vector}:
    \begin{equation*}
        \norm{\symbf{v}} = \sqrt{v_1^2 + v_2^2+\cdots+v_n^2}
    \end{equation*}
    \textbf{Unit vector}:
    \begin{equation*}
        \symbf{\hat{v}} = \frac{\symbf{v}}{\norm{\symbf{v}}}
    \end{equation*}
    \textbf{Dot product}:
    \begin{align*}
        \symbf{v}\cdot\symbf{w} & = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n                            \\
                                    & = \norm{\symbf{v}} \norm{\symbf{w}} \cos{\left(\theta\right)}
    \end{align*}
    \textbf{Cross product}:
    \begin{align*}
        \symbf{v}\times\symbf{w} & =
        \begin{vmatrix}
            \symbf{\hat{\imath}} & \symbf{\hat{\jmath}} & \symbf{\hat{k}} \\
            v_1                    & v_2                    & v_3               \\
            w_1                    & w_2                    & w_3
        \end{vmatrix} \\
                                     & =
        \norm{\symbf{v}}\norm{\symbf{w}}\sin{\left(\theta\right)}\symbf{\hat{n}}
    \end{align*}
    \textbf{2-d Inverse}:
    \begin{equation*}
        \begin{bmatrix}
            a & b \\ c & d
        \end{bmatrix}^{-1}
        = \frac{1}{ad - bc}
        \begin{bmatrix}
            d & -b \\ -c & a
        \end{bmatrix}
    \end{equation*}
    \section*{Vector Space Axioms}
    \textbf{Closure under addition}:
    \begin{equation*}
        \symbf{u}+\symbf{v} \in V
    \end{equation*}
    \textbf{Commutativity of vector addition}:
    \begin{equation*}
        \symbf{u} + \symbf{v} = \symbf{v} + \symbf{u}
    \end{equation*}
    \textbf{Associativity of vector addition}:
    \begin{equation*}
        \symbf{u} + \left(\symbf{v} + \symbf{w}\right) =
        \left(\symbf{u} + \symbf{v}\right) + \symbf{w}
    \end{equation*}
    \textbf{Additive identity}:
    \begin{equation*}
        \symbf{u} + \symbf{0} = \symbf{u}
    \end{equation*}
    \textbf{Additive inverse}:
    \begin{equation*}
        \symbf{u} + \left(-\symbf{u}\right) = \symbf{0}
    \end{equation*}
    \textbf{Closure under scalar multiplication}:
    \begin{equation*}
        k\symbf{u} \in V
    \end{equation*}
    \textbf{Distributivity of vector addition}:
    \begin{equation*}
        k \left(\symbf{u} + \symbf{v}\right) = k\symbf{u} + k\symbf{v}
    \end{equation*}
    \textbf{Distributivity of scalar addition}:
    \begin{equation*}
        \left(k+m\right)\symbf{u} = k\symbf{u} + m\symbf{u}
    \end{equation*}
    \textbf{Associativity of scalar multiplication}:
    \begin{equation*}
        k\left(m\symbf{u}\right)=\left(km\right)\symbf{u}
    \end{equation*}
    \textbf{Scalar multiplication identity}:
    \begin{equation*}
        1 \symbf{u}=\symbf{u}
    \end{equation*}
    \section*{Subspaces}
    \textbf{Subspaces of \(\mathbb{R}^2\)}: \(\left\{ \symbf{0} \right\}\),
    lines through the origin, and \(\mathbb{R}^2\).

    \textbf{Subspaces of \(\mathbb{R}^3\)}: \(\left\{ \symbf{0} \right\}\),
    lines through the origin, planes through the origin, and \(\mathbb{R}^3\).

    \textbf{Subspaces of \(\symbf{M}_{nn}\)}: Upper triangular matrices,
    lower triangular matrices, diagonal matrices, and \(\symbf{M}_{nn}\).
    \section*{Determinant Properties}
    \begin{enumerate}
        \item \(\det{\left( \symbf{I} \right)}=1\)
        \item Exchanging two rows of a matrix reverses the sign of its determinant
        \item Determinants are multilinear, so that
              \begin{equation*}
                  \begin{vmatrix}
                      a+a' & b+b' \\
                      c    & d
                  \end{vmatrix}
                  =
                  \begin{vmatrix}
                      a & b \\
                      c & d
                  \end{vmatrix}
                  +
                  \begin{vmatrix}
                      a' & b' \\
                      c  & d
                  \end{vmatrix}
              \end{equation*}
              and
              \begin{equation*}
                  \begin{vmatrix}
                      ta & tb \\
                      c  & d
                  \end{vmatrix} = t
                  \begin{vmatrix}
                      a & b \\
                      c & d
                  \end{vmatrix}
              \end{equation*}
        \item If \(\symbf{A}\) has two equal rows, then \(\det{\left( \symbf{A} \right)}=0\)
        \item Adding a scalar multiple of one row to another does not change the determinant of a matrix
        \item If \(\symbf{A}\) has a row of zeros, then \(\det{\left( \symbf{A} \right)}=0\)
        \item If \(\symbf{A}\) is triangular, then \(\det{\left( \symbf{A} \right)}=\prod_{i=1}^{n} a_{ii}\)
        \item If \(\symbf{A}\) is singular, then \(\det{\left( \symbf{A} \right)}=0\)
        \item \(\det{\left( \symbf{A}\symbf{B} \right)} = \det{\left( \symbf{A} \right)}\det{\left( \symbf{B} \right)}\)
        \item \(\det{\left( \symbf{A}^\top \right)} = \det{\left( \symbf{A} \right)}\)
    \end{enumerate}
    \section*{Matrix Identities}
    \begin{enumerate}
        \item \(\symbf{A}\left( \symbf{B}\symbf{C} \right) = \symbf{A}\symbf{B}+\symbf{A}\symbf{C}\)
        \item \(\left( \symbf{A}+\symbf{B} \right)^\top = \symbf{A}^\top + \symbf{B}^\top\)
        \item \(\left( \symbf{A}\symbf{B} \right)^\top = \symbf{B}^\top \symbf{A}^\top\)
        \item If \(\symbf{A}\) and \(\symbf{B}\) are both invertible:
              \begin{enumerate}
                  \item \(\left( \symbf{A}\symbf{B} \right)^{-1} = \symbf{B}^{-1}\symbf{A}^{-1}\)
                  \item \(\left( \symbf{A}^{-1} \right)^\top = \left( \symbf{A}^\top \right)^{-1}\)
              \end{enumerate}
    \end{enumerate}
\end{multicols*}
\end{document}
